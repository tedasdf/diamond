{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "560decb5",
   "metadata": {},
   "source": [
    "https://arxiv.org/pdf/1801.01290"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348ec8b8",
   "metadata": {},
   "source": [
    "# Soft Actor-Critic (SAC) Algorithm Implementation\n",
    "\n",
    "The environment is defined by the tuple $ (\\mathcal{S}, \\mathcal{A}, p, r)$, where:\n",
    "\n",
    "- $\\mathcal{S}$: State space  \n",
    "- $\\mathcal{A}$: Action space  \n",
    "- $p$: Transition probability  \n",
    "- $r$: Reward function  \n",
    "\n",
    "The components are defined as:\n",
    "\n",
    "- $p : \\mathcal{S} \\times \\mathcal{A} \\times \\mathcal{S} \\rightarrow [0, \\infty)$, representing the probability density of the next state given the current state and action.  \n",
    "- $r : \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathbb{R}$, representing the reward function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d147ece",
   "metadata": {},
   "source": [
    "OG reward :\n",
    "\n",
    "\n",
    "$$\n",
    "\\sum_{t} \\mathbb{E}_{(s_t, a_t) \\sim \\rho^\\pi} \\left[ r(s_t, a_t) \\right]\n",
    "$$\n",
    "\n",
    "\n",
    "reward function is :\n",
    "\n",
    "$$\n",
    "J(\\pi) = \\sum_{t=0}^{T} \\mathbb{E}_{(s_t, a_t) \\sim \\rho^\\pi} \\left[ r(s_t, a_t) + \\alpha \\mathcal{H}(\\pi(\\cdot \\mid s_t)) \\right]\n",
    "$$\n",
    "\n",
    "\n",
    "$ \\alpha $ determines the relative importance of the entropy term against the reward , thus control the stochasiticity of the policy\n",
    "\n",
    "\n",
    "\n",
    "## pros :\n",
    "\n",
    "- incentivized to explore more widely\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4618c3b",
   "metadata": {},
   "source": [
    "### actor ( the policy class )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a243b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "\n",
    "class Policy():\n",
    "\n",
    "    def __init__(\n",
    "        self , \n",
    "        action_space,\n",
    "        observation_space,\n",
    "        net_arch,\n",
    "        feature_extractor: nn.Module,\n",
    "        features_dim : int ,\n",
    "        use_sde : bool, # whether to use state dependent exploration\n",
    "        log_std_init,\n",
    "        full_std,\n",
    "        use_expln,\n",
    "        clip_mean,\n",
    "        normalize_images, ##???? # I am not sharing the feature extractor \n",
    "        activation_fn: type[nn.Module] = nn.ReLU,\n",
    "    ):\n",
    "        self.action_space = action_space\n",
    "        self.observation_space = observation_space\n",
    "        self.net_arch = net_arch\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.features_dim = features_dim\n",
    "        self.use_sde = use_sde\n",
    "        self.log_std_init = log_std_init\n",
    "        self.full_std = full_std\n",
    "        self.use_expln = use_expln\n",
    "        self.clip_mean = clip_mean\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f2bc28",
   "metadata": {},
   "source": [
    "### policy evaluation step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd839d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def sac_iter(\n",
    "        self,\n",
    "        replay_buffer,\n",
    "        env, \n",
    "        action,\n",
    "          ):\n",
    "    \n",
    "    for i in range(self.env_iter):\n",
    "        env_sample_function()\n",
    "\n",
    "    for i in range(self.gradient_iter):\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0521e915",
   "metadata": {},
   "outputs": [],
   "source": [
    "def  soft_policy(Q_function ,  reward_function , gamma , v_function):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4febacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(gradient_steps : int , batc_size: int = 6)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
