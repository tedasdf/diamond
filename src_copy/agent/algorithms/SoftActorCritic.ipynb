{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "348ec8b8",
   "metadata": {},
   "source": [
    "# Soft Actor-Critic (SAC) Algorithm Implementation\n",
    "\n",
    "The environment is defined by the tuple $ (\\mathcal{S}, \\mathcal{A}, p, r)$, where:\n",
    "\n",
    "- $\\mathcal{S}$: State space  \n",
    "- $\\mathcal{A}$: Action space  \n",
    "- $p$: Transition probability  \n",
    "- $r$: Reward function  \n",
    "\n",
    "The components are defined as:\n",
    "\n",
    "- $p : \\mathcal{S} \\times \\mathcal{A} \\times \\mathcal{S} \\rightarrow [0, \\infty)$, representing the probability density of the next state given the current state and action.  \n",
    "- $r : \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathbb{R}$, representing the reward function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d147ece",
   "metadata": {},
   "source": [
    "OG reward :\n",
    "\n",
    "\n",
    "$$\n",
    "\\sum_{t} \\mathbb{E}_{(s_t, a_t) \\sim \\rho^\\pi} \\left[ r(s_t, a_t) \\right]\n",
    "$$\n",
    "\n",
    "\n",
    "reward function is :\n",
    "\n",
    "$$\n",
    "J(\\pi) = \\sum_{t=0}^{T} \\mathbb{E}_{(s_t, a_t) \\sim \\rho^\\pi} \\left[ r(s_t, a_t) + \\alpha \\mathcal{H}(\\pi(\\cdot \\mid s_t)) \\right]\n",
    "$$\n",
    "\n",
    "\n",
    "$ \\alpha $ determines the relative importance of the entropy term against the reward , thus control the stochasiticity of the policy\n",
    "\n",
    "\n",
    "\n",
    "## pros :\n",
    "\n",
    "- incentivized to explore more widely\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f2bc28",
   "metadata": {},
   "source": [
    "### policy evaluation step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4febacb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
